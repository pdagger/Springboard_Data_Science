{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Classification with Naive Bayes\n",
    "***\n",
    "In the mini-project, you'll learn the basics of text analysis using a subset of movie reviews from the rotten tomatoes database. You'll also use a fundamental technique in Bayesian inference, called Naive Bayes. This mini-project is based on [Lab 10 of Harvard's CS109](https://github.com/cs109/2015lab10) class.  Please free to go to the original lab for additional exercises and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from six.moves import range\n",
    "\n",
    "# Setup Pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Setup Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Rotten Tomatoes Dataset](#Rotten-Tomatoes-Dataset)\n",
    "    * [Explore](#Explore)\n",
    "* [The Vector Space Model and a Search Engine](#The-Vector-Space-Model-and-a-Search-Engine)\n",
    "    * [In Code](#In-Code)\n",
    "* [Naive Bayes](#Naive-Bayes)\n",
    "    * [Multinomial Naive Bayes and Other Likelihood Functions](#Multinomial-Naive-Bayes-and-Other-Likelihood-Functions)\n",
    "    * [Picking Hyperparameters for Naive Bayes and Text Maintenance](#Picking-Hyperparameters-for-Naive-Bayes-and-Text-Maintenance)\n",
    "* [Interpretation](#Interpretation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh    imdb     publication                                              quote review_date  rtid      title\n",
       "1         Derek Adams  fresh  114709        Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story\n",
       "2     Richard Corliss  fresh  114709   TIME Magazine                  The year's most inventive comedy.  2008-08-31  9559  Toy story\n",
       "3         David Ansen  fresh  114709        Newsweek  A winning animated feature that has something ...  2008-08-18  9559  Toy story\n",
       "4       Leonard Klady  fresh  114709         Variety  The film sports a provocative and appealing st...  2008-06-09  9559  Toy story\n",
       "5  Jonathan Rosenbaum  fresh  114709  Chicago Reader  An entertaining computer-generated, hyperreali...  2008-03-10  9559  Toy story"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics = pd.read_csv('./critics.csv')\n",
    "#let's drop rows with missing quotes\n",
    "critics = critics[~critics.quote.isnull()]\n",
    "critics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 15561\n",
      "Number of critics: 623\n",
      "Number of movies:  1921\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len(critics)\n",
    "n_movies = critics.rtid.unique().size\n",
    "n_critics = critics.critic.unique().size\n",
    "\n",
    "\n",
    "print(\"Number of reviews: {:d}\".format(n_reviews))\n",
    "print(\"Number of critics: {:d}\".format(n_critics))\n",
    "print(\"Number of movies:  {:d}\".format(n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEYCAYAAAB2qXBEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68dcd5df60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = critics.copy()\n",
    "df['fresh'] = df.fresh == 'fresh'\n",
    "grp = df.groupby('critic')\n",
    "counts = grp.critic.count()  # number of reviews by each critic\n",
    "means = grp.fresh.mean()     # average freshness for each critic\n",
    "\n",
    "means[counts > 100].hist(bins=10, edgecolor='w', lw=1)\n",
    "plt.xlabel(\"Average Rating per critic\")\n",
    "plt.ylabel(\"Number of Critics\")\n",
    "plt.yticks([0, 2, 4, 6, 8, 10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set I</h3>\n",
    "<br/>\n",
    "<b>Exercise:</b> Look at the histogram above. Tell a story about the average ratings per critic. What shape does the distribution look like? What is interesting about the distribution? What might explain these interesting things?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most critics tend to give a positive rating.\n",
    "\n",
    "The histogram comes from a binary classification with categories 'fresh' or 'rotten', so the average rating per critic in reality means the percentage of films calified as 'fresh' per critic. Taking this into account, we se that most of the critics, 27 of 37, give a 'fresh' rating between 60% and 80% of the cases and only a minority of ten critics give a 'fresh' rating between 40% and 59% of whom only 6 give more 'roten' reviews than 'fresh' ones.\n",
    "\n",
    "The average rating per critic is around 0.6 and  the histogram distribution may be fitted to a gaussian distribution with a standard deviation of around 0.06."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space Model and a Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the diagrams here are snipped from [*Introduction to Information Retrieval* by Manning et. al.]( http://nlp.stanford.edu/IR-book/) which is a great resource on text processing. For additional information on text mining and natural language processing, see [*Foundations of Statistical Natural Language Processing* by Manning and Schutze](http://nlp.stanford.edu/fsnlp/).\n",
    "\n",
    "Also check out Python packages [`nltk`](http://www.nltk.org/), [`spaCy`](https://spacy.io/), [`pattern`](http://www.clips.ua.ac.be/pattern), and their associated resources. Also see [`word2vec`](https://en.wikipedia.org/wiki/Word2vec).\n",
    "\n",
    "Let us define the vector derived from document $d$ by $\\bar V(d)$. What does this mean? Each document is treated as a vector containing information about the words contained in it. Each vector has the same length and each entry \"slot\" in the vector contains some kind of data about the words that appear in the document such as presence/absence (1/0), count (an integer) or some other statistic. Each vector has the same length because each document shared the same vocabulary across the full collection of documents -- this collection is called a *corpus*.\n",
    "\n",
    "To define the vocabulary, we take a union of all words we have seen in all documents. We then just associate an array index with them. So \"hello\" may be at index 5 and \"world\" at index 99.\n",
    "\n",
    "Suppose we have the following corpus:\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox's mouth watered as he gazed longingly at them.`\n",
    "\n",
    "Suppose we treat each sentence as a document $d$. The vocabulary (often called the *lexicon*) is the following:\n",
    "\n",
    "$V = \\left\\{\\right.$ `a, along, and, as, at, beautiful, branches, bunch, burst, day, fox, fox's, from, gazed, grapes, hanging, he, juice, longingly, mouth, of, one, ready, ripe, seemed, spied, the, them, to, trained, tree, vine, watered, with`$\\left.\\right\\}$\n",
    "\n",
    "Then the document\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree`\n",
    "\n",
    "may be represented as the following sparse vector of word counts:\n",
    "\n",
    "$$\\bar V(d) = \\left( 4,1,0,0,0,1,1,1,0,1,1,0,1,0,1,1,0,0,0,0,2,1,0,1,0,0,1,0,0,0,1,1,0,0 \\right)$$\n",
    "\n",
    "or more succinctly as\n",
    "\n",
    "`[(0, 4), (1, 1), (5, 1), (6, 1), (7, 1), (9, 1), (10, 1), (12, 1), (14, 1), (15, 1), (20, 2), (21, 1), (23, 1),`\n",
    "`(26, 1), (30, 1), (31, 1)]`\n",
    "\n",
    "along with a dictionary\n",
    "\n",
    "``\n",
    "{\n",
    "    0: a, 1: along, 5: beautiful, 6: branches, 7: bunch, 9: day, 10: fox, 12: from, 14: grapes, \n",
    "    15: hanging, 19: mouth, 20: of, 21: one, 23: ripe, 24: seemed, 25: spied, 26: the, \n",
    "    30: tree, 31: vine, \n",
    "}\n",
    "``\n",
    "\n",
    "Then, a set of documents becomes, in the usual `sklearn` style, a sparse matrix with rows being sparse arrays representing documents and columns representing the features/words in the vocabulary.\n",
    "\n",
    "Notice that this representation loses the relative ordering of the terms in the document. That is \"cat ate rat\" and \"rat ate cat\" are the same. Thus, this representation is also known as the Bag-Of-Words representation.\n",
    "\n",
    "Here is another example, from the book quoted above, although the matrix is transposed here so that documents are columns:\n",
    "\n",
    "![novel terms](terms.png)\n",
    "\n",
    "Such a matrix is also catted a Term-Document Matrix. Here, the terms being indexed could be stemmed before indexing; for instance, `jealous` and `jealousy` after stemming are the same feature. One could also make use of other \"Natural Language Processing\" transformations in constructing the vocabulary. We could use Lemmatization, which reduces words to lemmas: work, working, worked would all reduce to work. We could remove \"stopwords\" from our vocabulary, such as common words like \"the\". We could look for particular parts of speech, such as adjectives. This is often done in Sentiment Analysis. And so on. It all depends on our application.\n",
    "\n",
    "From the book:\n",
    ">The standard way of quantifying the similarity between two documents $d_1$ and $d_2$  is to compute the cosine similarity of their vector representations $\\bar V(d_1)$ and $\\bar V(d_2)$:\n",
    "\n",
    "$$S_{12} = \\frac{\\bar V(d_1) \\cdot \\bar V(d_2)}{|\\bar V(d_1)| \\times |\\bar V(d_2)|}$$\n",
    "\n",
    "![Vector Space Model](vsm.png)\n",
    "\n",
    "\n",
    ">There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query q = jealous gossip. This query turns into the unit vector $\\bar V(q)$ = (0, 0.707, 0.707) on the three coordinates below. \n",
    "\n",
    "![novel terms](terms2.png)\n",
    "\n",
    ">The key idea now: to assign to each document d a score equal to the dot product:\n",
    "\n",
    "$$\\bar V(q) \\cdot \\bar V(d)$$\n",
    "\n",
    "Then we can use this simple Vector Model as a Search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text is\n",
      "Hop on pop\n",
      "Hop off pop\n",
      "Hop Hop hop\n",
      "\n",
      "Transformed text vector is \n",
      "[[1 0 1 1]\n",
      " [1 1 0 1]\n",
      " [3 0 0 0]]\n",
      "\n",
      "Words for each feature:\n",
      "['hop', 'off', 'on', 'pop']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['Hop on pop', 'Hop off pop', 'Hop Hop hop']\n",
    "print(\"Original text is\\n{}\".format('\\n'.join(text)))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(text)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Transformed text vector is \\n{}\".format(x))\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print(\"\")\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
    "# just their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy(critics, vectorizer=None):\n",
    "    #Your code here    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(critics.quote)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (critics.fresh == 'fresh').values.astype(np.int)\n",
    "    return X, y\n",
    "X, y = make_xy(critics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes' Theorem, we have that\n",
    "\n",
    "$$P(c \\vert f) = \\frac{P(c \\cap f)}{P(f)}$$\n",
    "\n",
    "where $c$ represents a *class* or category, and $f$ represents a feature vector, such as $\\bar V(d)$ as above. **We are computing the probability that a document (or whatever we are classifying) belongs to category *c* given the features in the document.** $P(f)$ is really just a normalization constant, so the literature usually writes Bayes' Theorem in context of Naive Bayes as\n",
    "\n",
    "$$P(c \\vert f) \\propto P(f \\vert c) P(c) $$\n",
    "\n",
    "$P(c)$ is called the *prior* and is simply the probability of seeing class $c$. But what is $P(f \\vert c)$? This is the probability that we see feature set $f$ given that this document is actually in class $c$. This is called the *likelihood* and comes from the data. One of the major assumptions of the Naive Bayes model is that the features are *conditionally independent* given the class. While the presence of a particular discriminative word may uniquely identify the document as being part of class $c$ and thus violate general feature independence, conditional independence means that the presence of that term is independent of all the other words that appear *within that class*. This is a very important distinction. Recall that if two events are independent, then:\n",
    "\n",
    "$$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
    "\n",
    "Thus, conditional independence implies\n",
    "\n",
    "$$P(f \\vert c)  = \\prod_i P(f_i | c) $$\n",
    "\n",
    "where $f_i$ is an individual feature (a word in this example).\n",
    "\n",
    "To make a classification, we then choose the class $c$ such that $P(c \\vert f)$ is maximal.\n",
    "\n",
    "There is a small caveat when computing these probabilities. For [floating point underflow](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html) we change the product into a sum by going into log space. This is called the LogSumExp trick. So:\n",
    "\n",
    "$$\\log P(f \\vert c)  = \\sum_i \\log P(f_i \\vert c) $$\n",
    "\n",
    "There is another caveat. What if we see a term that didn't exist in the training data? This means that $P(f_i \\vert c) = 0$ for that term, and thus $P(f \\vert c)  = \\prod_i P(f_i | c) = 0$, which doesn't help us at all. Instead of using zeros, we add a small negligible value called $\\alpha$ to each count. This is called Laplace Smoothing.\n",
    "\n",
    "$$P(f_i \\vert c) = \\frac{N_{ic}+\\alpha}{N_c + \\alpha N_i}$$\n",
    "\n",
    "where $N_{ic}$ is the number of times feature $i$ was seen in class $c$, $N_c$ is the number of times class $c$ was seen and $N_i$ is the number of times feature $i$ was seen globally. $\\alpha$ is sometimes called a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes and Other Likelihood Functions\n",
    "\n",
    "Since we are modeling word counts, we are using variation of Naive Bayes called Multinomial Naive Bayes. This is because the likelihood function actually takes the form of the multinomial distribution.\n",
    "\n",
    "$$P(f \\vert c) = \\frac{\\left( \\sum_i f_i \\right)!}{\\prod_i f_i!} \\prod_{f_i} P(f_i \\vert c)^{f_i} \\propto \\prod_{i} P(f_i \\vert c)$$\n",
    "\n",
    "where the nasty term out front is absorbed as a normalization constant such that probabilities sum to 1.\n",
    "\n",
    "There are many other variations of Naive Bayes, all which depend on what type of value $f_i$ takes. If $f_i$ is continuous, we may be able to use *Gaussian Naive Bayes*. First compute the mean and variance for each class $c$. Then the likelihood, $P(f \\vert c)$ is given as follows\n",
    "\n",
    "$$P(f_i = v \\vert c) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_c}} e^{- \\frac{\\left( v - \\mu_c \\right)^2}{2 \\sigma^2_c}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set II</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Implement a simple Naive Bayes classifier:</p>\n",
    "\n",
    "<ol>\n",
    "<li> split the data set into a training and test set\n",
    "<li> Use `scikit-learn`'s `MultinomialNB()` classifier with default parameters.\n",
    "<li> train the classifier over the training set and test on the test set\n",
    "<li> print the accuracy scores for both the training and the test sets\n",
    "</ol>\n",
    "\n",
    "What do you notice? Is this a good classifier? If not, why not?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9074\n",
      "Test accuracy: 0.7698\n"
     ]
    }
   ],
   "source": [
    "#your turn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "# Train model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "# Calculate accuracies\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(\"Training accuracy: %0.4f\" %train_accuracy)\n",
    "print(\"Test accuracy: %0.4f\" %test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy is greater than the test accuracy, but this is expected givent that the model should perform well in the training data.\n",
    "\n",
    "77% of accuracy in the test data seems a good core or at least an acceptable one but to really know if this is a good classifier we need first to define if we care about precission or recall and for that we need at take a look to the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1201  592]\n",
      " [ 483 2393]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the precision is: $2393 / (592 + 2393) \\approx 80.17\\%$.\n",
    "\n",
    "And the recall is: $2393 / (483 + 2393) \\approx 83.21\\%$.\n",
    "\n",
    "The numbers being over 80%, this seems a relatively good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Hyperparameters for Naive Bayes and Text Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what value to use for $\\alpha$, and we also need to know which words to include in the vocabulary. As mentioned earlier, some words are obvious stopwords. Other words appear so infrequently that they serve as noise, and other words in addition to stopwords appear so frequently that they may also serve as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find an appropriate value for `min_df` for the `CountVectorizer`. `min_df` can be either an integer or a float/decimal. If it is an integer, `min_df` represents the minimum number of documents a word must appear in for it to be included in the vocabulary. If it is a float, it represents the minimum *percentage* of documents a word must appear in to be included in the vocabulary. From the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set III</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Construct the cumulative distribution of document frequencies (df). The $x$-axis is a document count $x_i$ and the $y$-axis is the percentage of words that appear less than $x_i$ times. For example, at $x=5$, plot a point representing the percentage or number of words that appear in 5 or fewer documents.</p>\n",
    "\n",
    "<p><b>Exercise:</b> Look for the point at which the curve begins climbing steeply. This may be a good value for `min_df`. If we were interested in also picking `max_df`, we would likely pick the value where the curve starts to plateau. What value did you choose?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on csc_matrix in module scipy.sparse.csc object:\n",
      "\n",
      "class csc_matrix(scipy.sparse.compressed._cs_matrix, scipy.sparse.sputils.IndexMixin)\n",
      " |  Compressed Sparse Column matrix\n",
      " |  \n",
      " |  This can be instantiated in several ways:\n",
      " |  \n",
      " |      csc_matrix(D)\n",
      " |          with a dense matrix or rank-2 ndarray D\n",
      " |  \n",
      " |      csc_matrix(S)\n",
      " |          with another sparse matrix S (equivalent to S.tocsc())\n",
      " |  \n",
      " |      csc_matrix((M, N), [dtype])\n",
      " |          to construct an empty matrix with shape (M, N)\n",
      " |          dtype is optional, defaulting to dtype='d'.\n",
      " |  \n",
      " |      csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n",
      " |          where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n",
      " |          relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n",
      " |  \n",
      " |      csc_matrix((data, indices, indptr), [shape=(M, N)])\n",
      " |          is the standard CSC representation where the row indices for\n",
      " |          column i are stored in ``indices[indptr[i]:indptr[i+1]]``\n",
      " |          and their corresponding values are stored in\n",
      " |          ``data[indptr[i]:indptr[i+1]]``.  If the shape parameter is\n",
      " |          not supplied, the matrix dimensions are inferred from\n",
      " |          the index arrays.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  dtype : dtype\n",
      " |      Data type of the matrix\n",
      " |  shape : 2-tuple\n",
      " |      Shape of the matrix\n",
      " |  ndim : int\n",
      " |      Number of dimensions (this is always 2)\n",
      " |  nnz\n",
      " |      Number of nonzero elements\n",
      " |  data\n",
      " |      Data array of the matrix\n",
      " |  indices\n",
      " |      CSC format index array\n",
      " |  indptr\n",
      " |      CSC format index pointer array\n",
      " |  has_sorted_indices\n",
      " |      Whether indices are sorted\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  \n",
      " |  Sparse matrices can be used in arithmetic operations: they support\n",
      " |  addition, subtraction, multiplication, division, and matrix power.\n",
      " |  \n",
      " |  Advantages of the CSC format\n",
      " |      - efficient arithmetic operations CSC + CSC, CSC * CSC, etc.\n",
      " |      - efficient column slicing\n",
      " |      - fast matrix vector products (CSR, BSR may be faster)\n",
      " |  \n",
      " |  Disadvantages of the CSC format\n",
      " |    - slow row slicing operations (consider CSR)\n",
      " |    - changes to the sparsity structure are expensive (consider LIL or DOK)\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> import numpy as np\n",
      " |  >>> from scipy.sparse import csc_matrix\n",
      " |  >>> csc_matrix((3, 4), dtype=np.int8).toarray()\n",
      " |  array([[0, 0, 0, 0],\n",
      " |         [0, 0, 0, 0],\n",
      " |         [0, 0, 0, 0]], dtype=int8)\n",
      " |  \n",
      " |  >>> row = np.array([0, 2, 2, 0, 1, 2])\n",
      " |  >>> col = np.array([0, 0, 1, 2, 2, 2])\n",
      " |  >>> data = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> csc_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
      " |  array([[1, 0, 4],\n",
      " |         [0, 0, 5],\n",
      " |         [2, 3, 6]])\n",
      " |  \n",
      " |  >>> indptr = np.array([0, 2, 3, 6])\n",
      " |  >>> indices = np.array([0, 2, 2, 0, 1, 2])\n",
      " |  >>> data = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> csc_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n",
      " |  array([[1, 0, 4],\n",
      " |         [0, 0, 5],\n",
      " |         [2, 3, 6]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      csc_matrix\n",
      " |      scipy.sparse.compressed._cs_matrix\n",
      " |      scipy.sparse.data._data_matrix\n",
      " |      scipy.sparse.base.spmatrix\n",
      " |      scipy.sparse.data._minmax_mixin\n",
      " |      scipy.sparse.sputils.IndexMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  getcol(self, i)\n",
      " |      Returns a copy of column i of the matrix, as a (m x 1)\n",
      " |      CSC matrix (column vector).\n",
      " |  \n",
      " |  getrow(self, i)\n",
      " |      Returns a copy of row i of the matrix, as a (1 x n)\n",
      " |      CSR matrix (row vector).\n",
      " |  \n",
      " |  nonzero(self)\n",
      " |      nonzero indices\n",
      " |      \n",
      " |      Returns a tuple of arrays (row,col) containing the indices\n",
      " |      of the non-zero elements of the matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from scipy.sparse import csr_matrix\n",
      " |      >>> A = csr_matrix([[1,2,0],[0,0,3],[4,0,5]])\n",
      " |      >>> A.nonzero()\n",
      " |      (array([0, 0, 1, 2, 2]), array([0, 1, 2, 0, 2]))\n",
      " |  \n",
      " |  tocsc(self, copy=False)\n",
      " |      Convert this matrix to Compressed Sparse Column format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant csc_matrix.\n",
      " |  \n",
      " |  tocsr(self, copy=False)\n",
      " |      Convert this matrix to Compressed Sparse Row format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant csr_matrix.\n",
      " |  \n",
      " |  transpose(self, axes=None, copy=False)\n",
      " |      Reverses the dimensions of the sparse matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axes : None, optional\n",
      " |          This argument is in the signature *solely* for NumPy\n",
      " |          compatibility reasons. Do not pass in anything except\n",
      " |          for the default value.\n",
      " |      copy : bool, optional\n",
      " |          Indicates whether or not attributes of `self` should be\n",
      " |          copied whenever possible. The degree to which attributes\n",
      " |          are copied varies depending on the type of sparse matrix\n",
      " |          being used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : `self` with the dimensions reversed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      np.matrix.transpose : NumPy's implementation of 'transpose'\n",
      " |                            for matrices\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  format = 'csc'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from scipy.sparse.compressed._cs_matrix:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, arg1, shape=None, dtype=None, copy=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setitem__(self, index, x)\n",
      " |  \n",
      " |  check_format(self, full_check=True)\n",
      " |      check whether the matrix format is valid\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      full_check : bool, optional\n",
      " |          If `True`, rigorous check, O(N) operations. Otherwise\n",
      " |          basic check, O(1) operations (default True).\n",
      " |  \n",
      " |  diagonal(self, k=0)\n",
      " |      Returns the k-th diagonal of the matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      k : int, optional\n",
      " |          Which diagonal to set, corresponding to elements a[i, i+k].\n",
      " |          Default: 0 (the main diagonal).\n",
      " |      \n",
      " |          .. versionadded: 1.0\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.diagonal : Equivalent numpy function.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from scipy.sparse import csr_matrix\n",
      " |      >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
      " |      >>> A.diagonal()\n",
      " |      array([1, 0, 5])\n",
      " |      >>> A.diagonal(k=1)\n",
      " |      array([2, 3])\n",
      " |  \n",
      " |  eliminate_zeros(self)\n",
      " |      Remove zero entries from the matrix\n",
      " |      \n",
      " |      This is an *in place* operation\n",
      " |  \n",
      " |  getnnz(self, axis=None)\n",
      " |      Number of stored values, including explicit zeros.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : None, 0, or 1\n",
      " |          Select between the number of values across the whole matrix, in\n",
      " |          each column, or in each row.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      count_nonzero : Number of non-zero entries\n",
      " |  \n",
      " |  maximum(self, other)\n",
      " |      Element-wise maximum between this and another matrix.\n",
      " |  \n",
      " |  minimum(self, other)\n",
      " |      Element-wise minimum between this and another matrix.\n",
      " |  \n",
      " |  multiply(self, other)\n",
      " |      Point-wise multiplication by another matrix, vector, or\n",
      " |      scalar.\n",
      " |  \n",
      " |  prune(self)\n",
      " |      Remove empty space after all non-zero elements.\n",
      " |  \n",
      " |  sort_indices(self)\n",
      " |      Sort the indices of this matrix *in place*\n",
      " |  \n",
      " |  sorted_indices(self)\n",
      " |      Return a copy of this matrix with sorted indices\n",
      " |  \n",
      " |  sum(self, axis=None, dtype=None, out=None)\n",
      " |      Sum the matrix elements over a given axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {-2, -1, 0, 1, None} optional\n",
      " |          Axis along which the sum is computed. The default is to\n",
      " |          compute the sum of all the matrix elements, returning a scalar\n",
      " |          (i.e. `axis` = `None`).\n",
      " |      dtype : dtype, optional\n",
      " |          The type of the returned matrix and of the accumulator in which\n",
      " |          the elements are summed.  The dtype of `a` is used by default\n",
      " |          unless `a` has an integer dtype of less precision than the default\n",
      " |          platform integer.  In that case, if `a` is signed then the platform\n",
      " |          integer is used while if `a` is unsigned then an unsigned integer\n",
      " |          of the same precision as the platform integer is used.\n",
      " |      \n",
      " |          .. versionadded: 0.18.0\n",
      " |      \n",
      " |      out : np.matrix, optional\n",
      " |          Alternative output matrix in which to place the result. It must\n",
      " |          have the same shape as the expected output, but the type of the\n",
      " |          output values will be cast if necessary.\n",
      " |      \n",
      " |          .. versionadded: 0.18.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sum_along_axis : np.matrix\n",
      " |          A matrix with the same shape as `self`, with the specified\n",
      " |          axis removed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      np.matrix.sum : NumPy's implementation of 'sum' for matrices\n",
      " |  \n",
      " |  sum_duplicates(self)\n",
      " |      Eliminate duplicate matrix entries by adding them together\n",
      " |      \n",
      " |      The is an *in place* operation\n",
      " |  \n",
      " |  toarray(self, order=None, out=None)\n",
      " |      Return a dense ndarray representation of this matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F'}, optional\n",
      " |          Whether to store multi-dimensional data in C (row-major)\n",
      " |          or Fortran (column-major) order in memory. The default\n",
      " |          is 'None', indicating the NumPy default of C-ordered.\n",
      " |          Cannot be specified in conjunction with the `out`\n",
      " |          argument.\n",
      " |      \n",
      " |      out : ndarray, 2-dimensional, optional\n",
      " |          If specified, uses this array as the output buffer\n",
      " |          instead of allocating a new array to return. The provided\n",
      " |          array must have the same shape and dtype as the sparse\n",
      " |          matrix on which you are calling the method. For most\n",
      " |          sparse types, `out` is required to be memory contiguous\n",
      " |          (either C or Fortran ordered).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      arr : ndarray, 2-dimensional\n",
      " |          An array with the same shape and containing the same\n",
      " |          data represented by the sparse matrix, with the requested\n",
      " |          memory order. If `out` was passed, the same object is\n",
      " |          returned after being modified in-place to contain the\n",
      " |          appropriate values.\n",
      " |  \n",
      " |  tocoo(self, copy=True)\n",
      " |      Convert this matrix to COOrdinate format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant coo_matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from scipy.sparse.compressed._cs_matrix:\n",
      " |  \n",
      " |  has_canonical_format\n",
      " |      Determine whether the matrix has sorted indices and no duplicates\n",
      " |      \n",
      " |      Returns\n",
      " |          - True: if the above applies\n",
      " |          - False: otherwise\n",
      " |      \n",
      " |      has_canonical_format implies has_sorted_indices, so if the latter flag\n",
      " |      is False, so will the former be; if the former is found True, the\n",
      " |      latter flag is also set.\n",
      " |  \n",
      " |  has_sorted_indices\n",
      " |      Determine whether the matrix has sorted indices\n",
      " |      \n",
      " |      Returns\n",
      " |          - True: if the indices of the matrix are in sorted order\n",
      " |          - False: otherwise\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from scipy.sparse.compressed._cs_matrix:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from scipy.sparse.data._data_matrix:\n",
      " |  \n",
      " |  __abs__(self)\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |  \n",
      " |  __itruediv__(self, other)\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |  \n",
      " |  arcsin(self)\n",
      " |      Element-wise arcsin.\n",
      " |      \n",
      " |      See numpy.arcsin for more information.\n",
      " |  \n",
      " |  arcsinh(self)\n",
      " |      Element-wise arcsinh.\n",
      " |      \n",
      " |      See numpy.arcsinh for more information.\n",
      " |  \n",
      " |  arctan(self)\n",
      " |      Element-wise arctan.\n",
      " |      \n",
      " |      See numpy.arctan for more information.\n",
      " |  \n",
      " |  arctanh(self)\n",
      " |      Element-wise arctanh.\n",
      " |      \n",
      " |      See numpy.arctanh for more information.\n",
      " |  \n",
      " |  astype(self, dtype, casting='unsafe', copy=True)\n",
      " |      Cast the matrix elements to a specified type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : string or numpy dtype\n",
      " |          Typecode or data-type to which to cast the data.\n",
      " |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      " |          Controls what kind of data casting may occur.\n",
      " |          Defaults to 'unsafe' for backwards compatibility.\n",
      " |          'no' means the data types should not be cast at all.\n",
      " |          'equiv' means only byte-order changes are allowed.\n",
      " |          'safe' means only casts which can preserve values are allowed.\n",
      " |          'same_kind' means only safe casts or casts within a kind,\n",
      " |          like float64 to float32, are allowed.\n",
      " |          'unsafe' means any data conversions may be done.\n",
      " |      copy : bool, optional\n",
      " |          If `copy` is `False`, the result might share some memory with this\n",
      " |          matrix. If `copy` is `True`, it is guaranteed that the result and\n",
      " |          this matrix do not share any memory.\n",
      " |  \n",
      " |  ceil(self)\n",
      " |      Element-wise ceil.\n",
      " |      \n",
      " |      See numpy.ceil for more information.\n",
      " |  \n",
      " |  conj(self)\n",
      " |      Element-wise complex conjugation.\n",
      " |      \n",
      " |      If the matrix is of non-complex data type, then this method does\n",
      " |      nothing and the data is not copied.\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Returns a copy of this matrix.\n",
      " |      \n",
      " |      No data/indices will be shared between the returned value and current\n",
      " |      matrix.\n",
      " |  \n",
      " |  count_nonzero(self)\n",
      " |      Number of non-zero entries, equivalent to\n",
      " |      \n",
      " |      np.count_nonzero(a.toarray())\n",
      " |      \n",
      " |      Unlike getnnz() and the nnz property, which return the number of stored\n",
      " |      entries (the length of the data attribute), this method counts the\n",
      " |      actual number of non-zero entries in data.\n",
      " |  \n",
      " |  deg2rad(self)\n",
      " |      Element-wise deg2rad.\n",
      " |      \n",
      " |      See numpy.deg2rad for more information.\n",
      " |  \n",
      " |  expm1(self)\n",
      " |      Element-wise expm1.\n",
      " |      \n",
      " |      See numpy.expm1 for more information.\n",
      " |  \n",
      " |  floor(self)\n",
      " |      Element-wise floor.\n",
      " |      \n",
      " |      See numpy.floor for more information.\n",
      " |  \n",
      " |  log1p(self)\n",
      " |      Element-wise log1p.\n",
      " |      \n",
      " |      See numpy.log1p for more information.\n",
      " |  \n",
      " |  power(self, n, dtype=None)\n",
      " |      This function performs element-wise power.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : n is a scalar\n",
      " |      \n",
      " |      dtype : If dtype is not specified, the current dtype will be preserved.\n",
      " |  \n",
      " |  rad2deg(self)\n",
      " |      Element-wise rad2deg.\n",
      " |      \n",
      " |      See numpy.rad2deg for more information.\n",
      " |  \n",
      " |  rint(self)\n",
      " |      Element-wise rint.\n",
      " |      \n",
      " |      See numpy.rint for more information.\n",
      " |  \n",
      " |  sign(self)\n",
      " |      Element-wise sign.\n",
      " |      \n",
      " |      See numpy.sign for more information.\n",
      " |  \n",
      " |  sin(self)\n",
      " |      Element-wise sin.\n",
      " |      \n",
      " |      See numpy.sin for more information.\n",
      " |  \n",
      " |  sinh(self)\n",
      " |      Element-wise sinh.\n",
      " |      \n",
      " |      See numpy.sinh for more information.\n",
      " |  \n",
      " |  sqrt(self)\n",
      " |      Element-wise sqrt.\n",
      " |      \n",
      " |      See numpy.sqrt for more information.\n",
      " |  \n",
      " |  tan(self)\n",
      " |      Element-wise tan.\n",
      " |      \n",
      " |      See numpy.tan for more information.\n",
      " |  \n",
      " |  tanh(self)\n",
      " |      Element-wise tanh.\n",
      " |      \n",
      " |      See numpy.tanh for more information.\n",
      " |  \n",
      " |  trunc(self)\n",
      " |      Element-wise trunc.\n",
      " |      \n",
      " |      See numpy.trunc for more information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from scipy.sparse.data._data_matrix:\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from scipy.sparse.base.spmatrix:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __div__(self, other)\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |  \n",
      " |  __idiv__(self, other)\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      # What should len(sparse) return? For consistency with dense matrices,\n",
      " |      # perhaps it should be the number of rows?  But for some uses the number of\n",
      " |      # non-zeros is more important.  For now, raise an exception!\n",
      " |  \n",
      " |  __matmul__(self, other)\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |      interpret other and call one of the following\n",
      " |      \n",
      " |      self._mul_scalar()\n",
      " |      self._mul_vector()\n",
      " |      self._mul_multivector()\n",
      " |      self._mul_sparse_matrix()\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __numpy_ufunc__(self, func, method, pos, inputs, **kwargs)\n",
      " |      Method for compatibility with NumPy's ufuncs and dot\n",
      " |      functions.\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmatmul__(self, other)\n",
      " |  \n",
      " |  __rmul__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__(self, other)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |  \n",
      " |  __truediv__(self, other)\n",
      " |  \n",
      " |  asformat(self, format)\n",
      " |      Return this matrix in a given sparse format\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      format : {string, None}\n",
      " |          desired sparse matrix format\n",
      " |              - None for no format conversion\n",
      " |              - \"csr\" for csr_matrix format\n",
      " |              - \"csc\" for csc_matrix format\n",
      " |              - \"lil\" for lil_matrix format\n",
      " |              - \"dok\" for dok_matrix format and so on\n",
      " |  \n",
      " |  asfptype(self)\n",
      " |      Upcast matrix to a floating point format (if necessary)\n",
      " |  \n",
      " |  conjugate(self)\n",
      " |      Element-wise complex conjugation.\n",
      " |      \n",
      " |      If the matrix is of non-complex data type, then this method does\n",
      " |      nothing and the data is not copied.\n",
      " |  \n",
      " |  dot(self, other)\n",
      " |      Ordinary dot product\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from scipy.sparse import csr_matrix\n",
      " |      >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
      " |      >>> v = np.array([1, 0, -1])\n",
      " |      >>> A.dot(v)\n",
      " |      array([ 1, -3, -1], dtype=int64)\n",
      " |  \n",
      " |  getH(self)\n",
      " |      Return the Hermitian transpose of this matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      np.matrix.getH : NumPy's implementation of `getH` for matrices\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Get shape of a matrix.\n",
      " |  \n",
      " |  getformat(self)\n",
      " |      Format of a matrix representation as a string.\n",
      " |  \n",
      " |  getmaxprint(self)\n",
      " |      Maximum number of elements to display when printed.\n",
      " |  \n",
      " |  mean(self, axis=None, dtype=None, out=None)\n",
      " |      Compute the arithmetic mean along the specified axis.\n",
      " |      \n",
      " |      Returns the average of the matrix elements. The average is taken\n",
      " |      over all elements in the matrix by default, otherwise over the\n",
      " |      specified axis. `float64` intermediate and return values are used\n",
      " |      for integer inputs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {-2, -1, 0, 1, None} optional\n",
      " |          Axis along which the mean is computed. The default is to compute\n",
      " |          the mean of all elements in the matrix (i.e. `axis` = `None`).\n",
      " |      dtype : data-type, optional\n",
      " |          Type to use in computing the mean. For integer inputs, the default\n",
      " |          is `float64`; for floating point inputs, it is the same as the\n",
      " |          input dtype.\n",
      " |      \n",
      " |          .. versionadded: 0.18.0\n",
      " |      \n",
      " |      out : np.matrix, optional\n",
      " |          Alternative output matrix in which to place the result. It must\n",
      " |          have the same shape as the expected output, but the type of the\n",
      " |          output values will be cast if necessary.\n",
      " |      \n",
      " |          .. versionadded: 0.18.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      m : np.matrix\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      np.matrix.mean : NumPy's implementation of 'mean' for matrices\n",
      " |  \n",
      " |  reshape(self, shape, order='C')\n",
      " |      Gives a new shape to a sparse matrix without changing its data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      shape : length-2 tuple of ints\n",
      " |          The new shape should be compatible with the original shape.\n",
      " |      order : 'C', optional\n",
      " |          This argument is in the signature *solely* for NumPy\n",
      " |          compatibility reasons. Do not pass in anything except\n",
      " |          for the default value, as this argument is not used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reshaped_matrix : `self` with the new dimensions of `shape`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      np.matrix.reshape : NumPy's implementation of 'reshape' for matrices\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      See `reshape`.\n",
      " |  \n",
      " |  setdiag(self, values, k=0)\n",
      " |      Set diagonal or off-diagonal elements of the array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      values : array_like\n",
      " |          New values of the diagonal elements.\n",
      " |      \n",
      " |          Values may have any length.  If the diagonal is longer than values,\n",
      " |          then the remaining diagonal entries will not be set.  If values if\n",
      " |          longer than the diagonal, then the remaining values are ignored.\n",
      " |      \n",
      " |          If a scalar value is given, all of the diagonal is set to it.\n",
      " |      \n",
      " |      k : int, optional\n",
      " |          Which off-diagonal to set, corresponding to elements a[i,i+k].\n",
      " |          Default: 0 (the main diagonal).\n",
      " |  \n",
      " |  tobsr(self, blocksize=None, copy=False)\n",
      " |      Convert this matrix to Block Sparse Row format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant bsr_matrix.\n",
      " |      \n",
      " |      When blocksize=(R, C) is provided, it will be used for construction of\n",
      " |      the bsr_matrix.\n",
      " |  \n",
      " |  todense(self, order=None, out=None)\n",
      " |      Return a dense matrix representation of this matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F'}, optional\n",
      " |          Whether to store multi-dimensional data in C (row-major)\n",
      " |          or Fortran (column-major) order in memory. The default\n",
      " |          is 'None', indicating the NumPy default of C-ordered.\n",
      " |          Cannot be specified in conjunction with the `out`\n",
      " |          argument.\n",
      " |      \n",
      " |      out : ndarray, 2-dimensional, optional\n",
      " |          If specified, uses this array (or `numpy.matrix`) as the\n",
      " |          output buffer instead of allocating a new array to\n",
      " |          return. The provided array must have the same shape and\n",
      " |          dtype as the sparse matrix on which you are calling the\n",
      " |          method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      arr : numpy.matrix, 2-dimensional\n",
      " |          A NumPy matrix object with the same shape and containing\n",
      " |          the same data represented by the sparse matrix, with the\n",
      " |          requested memory order. If `out` was passed and was an\n",
      " |          array (rather than a `numpy.matrix`), it will be filled\n",
      " |          with the appropriate values and returned wrapped in a\n",
      " |          `numpy.matrix` object that shares the same memory.\n",
      " |  \n",
      " |  todia(self, copy=False)\n",
      " |      Convert this matrix to sparse DIAgonal format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant dia_matrix.\n",
      " |  \n",
      " |  todok(self, copy=False)\n",
      " |      Convert this matrix to Dictionary Of Keys format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant dok_matrix.\n",
      " |  \n",
      " |  tolil(self, copy=False)\n",
      " |      Convert this matrix to LInked List format.\n",
      " |      \n",
      " |      With copy=False, the data/indices may be shared between this matrix and\n",
      " |      the resultant lil_matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from scipy.sparse.base.spmatrix:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  nnz\n",
      " |      Number of stored values, including explicit zeros.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      count_nonzero : Number of non-zero entries\n",
      " |  \n",
      " |  shape\n",
      " |      Get shape of a matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from scipy.sparse.base.spmatrix:\n",
      " |  \n",
      " |  __array_priority__ = 10.1\n",
      " |  \n",
      " |  ndim = 2\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from scipy.sparse.data._minmax_mixin:\n",
      " |  \n",
      " |  argmax(self, axis=None, out=None)\n",
      " |      Return indices of maximum elements along an axis.\n",
      " |      \n",
      " |      Implicit zero elements are also taken into account. If there are\n",
      " |      several maximum values, the index of the first occurrence is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {-2, -1, 0, 1, None}, optional\n",
      " |          Axis along which the argmax is computed. If None (default), index\n",
      " |          of the maximum element in the flatten data is returned.\n",
      " |      out : None, optional\n",
      " |          This argument is in the signature *solely* for NumPy\n",
      " |          compatibility reasons. Do not pass in anything except for\n",
      " |          the default value, as this argument is not used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ind : np.matrix or int\n",
      " |          Indices of maximum elements. If matrix, its size along `axis` is 1.\n",
      " |  \n",
      " |  argmin(self, axis=None, out=None)\n",
      " |      Return indices of minimum elements along an axis.\n",
      " |      \n",
      " |      Implicit zero elements are also taken into account. If there are\n",
      " |      several minimum values, the index of the first occurrence is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {-2, -1, 0, 1, None}, optional\n",
      " |          Axis along which the argmin is computed. If None (default), index\n",
      " |          of the minimum element in the flatten data is returned.\n",
      " |      out : None, optional\n",
      " |          This argument is in the signature *solely* for NumPy\n",
      " |          compatibility reasons. Do not pass in anything except for\n",
      " |          the default value, as this argument is not used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |       ind : np.matrix or int\n",
      " |          Indices of minimum elements. If matrix, its size along `axis` is 1.\n",
      " |  \n",
      " |  max(self, axis=None, out=None)\n",
      " |      Return the maximum of the matrix or maximum along an axis.\n",
      " |      This takes all elements into account, not just the non-zero ones.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {-2, -1, 0, 1, None} optional\n",
      " |          Axis along which the sum is computed. The default is to\n",
      " |          compute the maximum over all the matrix elements, returning\n",
      " |          a scalar (i.e. `axis` = `None`).\n",
      " |      \n",
      " |      out : None, optional\n",
      " |          This argument is in the signature *solely* for NumPy\n",
      " |          compatibility reasons. Do not pass in anything except\n",
      " |          for the default value, as this argument is not used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      amax : coo_matrix or scalar\n",
      " |          Maximum of `a`. If `axis` is None, the result is a scalar value.\n",
      " |          If `axis` is given, the result is a sparse.coo_matrix of dimension\n",
      " |          ``a.ndim - 1``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      min : The minimum value of a sparse matrix along a given axis.\n",
      " |      np.matrix.max : NumPy's implementation of 'max' for matrices\n",
      " |  \n",
      " |  min(self, axis=None, out=None)\n",
      " |      Return the minimum of the matrix or maximum along an axis.\n",
      " |      This takes all elements into account, not just the non-zero ones.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {-2, -1, 0, 1, None} optional\n",
      " |          Axis along which the sum is computed. The default is to\n",
      " |          compute the minimum over all the matrix elements, returning\n",
      " |          a scalar (i.e. `axis` = `None`).\n",
      " |      \n",
      " |      out : None, optional\n",
      " |          This argument is in the signature *solely* for NumPy\n",
      " |          compatibility reasons. Do not pass in anything except for\n",
      " |          the default value, as this argument is not used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      amin : coo_matrix or scalar\n",
      " |          Minimum of `a`. If `axis` is None, the result is a scalar value.\n",
      " |          If `axis` is given, the result is a sparse.coo_matrix of dimension\n",
      " |          ``a.ndim - 1``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      max : The maximum value of a sparse matrix along a given axis.\n",
      " |      np.matrix.min : NumPy's implementation of 'min' for matrices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your turn.\n",
    "help(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAFjCAYAAABfZS1yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68d5822fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get boolean matrix for values > 0\n",
    "# (a column is a word, a row is a document)\n",
    "# sum along columns to get in how many documents a word appears \n",
    "word_num_documents = np.sort(np.array((X>0).sum(axis=0))[0])\n",
    "words = X.shape[1]\n",
    "documents = X.shape[0]\n",
    "word_freq = np.sort(np.array((X>0).sum(axis=0))[0]) / documents\n",
    "percentage = np.arange(1, words+1) / words * 100\n",
    "# plot\n",
    "plt.figure(figsize=(10,5))\n",
    "# plt.plot(word_num_documents, percentage, marker='.', linestyle='none')\n",
    "plt.hist(word_num_documents, bins=100, cumulative=True, histtype='step', normed=True, color='b')\n",
    "plt.xlabel(\"Number of documents\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Cumulative distribution of document frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve is steeply from the beginning and reaches a plateau fairly fast, let's take a closer look to the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAFjCAYAAABfZS1yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68d583eeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "lim = 20000\n",
    "# plt.plot(word_num_documents[:lim], percentage[:lim], marker='.', linestyle='none')\n",
    "plt.hist(word_num_documents, bins=100, range=(1,50), cumulative=True, histtype='step', normed=True)\n",
    "plt.xlabel(\"Number of documents\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Cumulative distribution of document frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\alpha$ is chosen to be a small value that simply avoids having zeros in the probability computations. This value can sometimes be chosen arbitrarily with domain expertise, but we will use K-fold cross validation. In K-fold cross-validation, we divide the data into $K$ non-overlapping parts. We train on $K-1$ of the folds and test on the remaining fold. We then iterate, so that each fold serves as the test fold exactly once. The function `cv_score` performs the K-fold cross-validation algorithm for us, but we need to pass a function that measures the performance of the algorithm on each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-likelihood as the score here in `scorefunc`. The higher the log-likelihood, the better. Indeed, what we do in `cv_score` above is to implement the cross-validation part of `GridSearchCV`.\n",
    "\n",
    "The custom scoring function `scorefunc` allows us to use different metrics depending on the decision risk we care about (precision, accuracy, profit etc.) directly on the validation set. You will often find people using `roc_auc`, precision, recall, or `F1-score` as the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cross-validate over the regularization parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the train and test masks first, and then we can run the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfgr/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\n",
    "mask = np.zeros(critics.shape[0], dtype=np.bool)\n",
    "mask[itest] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set IV</h3>\n",
    "\n",
    "<p><b>Exercise:</b> What does using the function `log_likelihood` as the score mean? What are we trying to optimize for?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Without writing any code, what do you think would happen if you choose a value of $\\alpha$ that is too high?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Using the skeleton code below, find the best values of the parameter `alpha`, and use the value of `min_df` you chose in the previous exercise set. Use the `cv_score` function above with the `log_likelihood` function for scoring.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_likelihood gives  the logarithm of the total probability that the labels ('fresh' or 'rotten) are reproduced by the model, or in other words it represents the logarithm of the probability of the correct labelling given the model. In this case we want to optimize the log_likelihood because we are going to determine the model that has the higher chances to infere the correct labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of a feature is:\n",
    "\n",
    "$$P(f_i \\vert c) = \\frac{N_{ic}+\\alpha}{N_c + \\alpha N_i}$$\n",
    "\n",
    "If $\\alpha$ is too high $N_{ic}$ and $N_c$ will become negligeable in the numerator and denominator respectively, then the likelihood of a feature will tend to $1/N_i$. The total likelihood of the proper labelling of the data given the model that is $P(f \\vert c) \\propto \\prod_{i} P(f_i \\vert c)$ will be then tend to $\\prod_{i} 1/N_i$ where $N_i$ is the number of times feature $f_i$ is seen globally. In other words, if $\\alpha$ is too high the likelihood will tend to the product of the inverse of the number of times each word has been seen in the data set and the log_likelihood will tend to the negative summation of the logarithms of the number of times each word appears in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]\n",
    "best_min_df = min_df # YOUR TURN: put your value of min_df here.\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = CountVectorizer(min_df=best_min_df)       \n",
    "    Xthis, ythis = make_xy(critics, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    # your turn\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    score = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    if score > maxscore:\n",
    "        best_alpha = alpha\n",
    "        maxscore = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"alpha: {}\".format(best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set V: Working with the Best Parameters</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Using the best value of  `alpha` you just found, calculate the accuracy on the training and test sets. Is this classifier better? Why (not)?</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.932748\n",
      "Accuracy on test data:     0.742839\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "X, y = make_xy(critics, vectorizer)\n",
    "xtrain=X[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "#your turn. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2314 1958]\n",
      " [ 843 5777]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, clf.predict(xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a better training accuracy, 93.06% against 90.74% for the previous model, so we are tempted to say that this classifier is better than the previous one. However the testing accuracy of 74.72% is worst than 76.98% for the previous model. I will tend to give a higher weight to the testing accuracy but to be sure let's now compare the precission and recall numbers.\n",
    "\n",
    "As a reminder the precision and recall for the previous model are 80.17% and 83.21% respectively.\n",
    "\n",
    "For this model we have:\n",
    "\n",
    "Accuracy $= 5777 / (1958 + 5777) \\approx 74.69\\%$.\n",
    "\n",
    "Recall $= 5777 / (843 + 577) \\approx 87.27\\%$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the accuracy the previous model is better but if we consider the recall this classifier is defitively better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the strongly predictive features?\n",
    "\n",
    "We use a neat trick to identify strongly predictive features (i.e. words). \n",
    "\n",
    "* first, create a data set such that each row has exactly one feature. This is represented by the identity matrix.\n",
    "* use the trained classifier to make predictions on this matrix\n",
    "* sort the rows by predicted probabilities, and pick the top and bottom $K$ rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(fresh | word)\n",
      "             perfect 0.97\n",
      "            pleasure 0.96\n",
      "             kubrick 0.95\n",
      "          surprising 0.95\n",
      "               crime 0.95\n",
      "              superb 0.95\n",
      "             delight 0.94\n",
      "              forget 0.93\n",
      "        irresistible 0.93\n",
      "           absorbing 0.93\n",
      "Bad words\t     P(fresh | word)\n",
      "       disappointing 0.12\n",
      "              unless 0.12\n",
      "              mildly 0.12\n",
      "        dramatically 0.12\n",
      "             problem 0.09\n",
      "             unfunny 0.08\n",
      "          uninspired 0.08\n",
      "           pointless 0.07\n",
      "                dull 0.06\n",
      "       unfortunately 0.05\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(xtest.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(fresh | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(fresh | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VI</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Why does this method work? What does the probability for each row in the identity matrix represent</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works because the model learns to associate each word with a good or bad meaning.\n",
    "\n",
    "The probability of each row in the \"identity matrix\" corresponds to the probability of each word beaing a positive word. In this context positive words have a probability close to 1 and negaitve words a probability close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above exercise is an example of *feature selection*. There are many other feature selection methods. A list of feature selection methods available in `sklearn` is [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection). The most common feature selection technique for text mining is the chi-squared $\\left( \\chi^2 \\right)$ [method](http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Errors\n",
    "\n",
    "We can see mis-predictions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mis-predicted Rotten quotes\n",
      "---------------------------\n",
      "As a depiction of a loving-turbulent relationship between a single mom (Susan Sarandon) and her rebellious teenage daughter (Natalie Portman), Wang's meller is nicely crafted but old-fashioned like Hollywood's weepies of yesteryear.\n",
      "\n",
      "A bleak, annoyingly quirky Gen-X recasting of the When Harry Met Sally notion that true love is based on friendship.\n",
      "\n",
      "What if this lesser-known chapter of German resistance had been more deeply captured? What if the moral conflicts running through this movie about love of country and revolt said more about Germany, war and, yes, genocide?\n",
      "\n",
      "Herzog offers some evidence of Kinski's great human warmth, somewhat more of his rage of unimaginable proportions, and a good demonstration of Kinski's uncanny capacity to corkscrew his way into the frame.\n",
      "\n",
      "At best, it is fun. But \"fun\" is not an aesthetic experience: fun remains on the surface. I have nothing against the surface. But it belongs where it is and shouldn't be taken for anything else.\n",
      "\n",
      "Mis-predicted Fresh quotes\n",
      "--------------------------\n",
      "It's inanely-scripted exploitation, sure, but this 'Halloween' doesn't trivialise; it even returns with sympathy to one victim minutes after the attack that has left her bleeding on the floor.\n",
      "\n",
      "Primary Colors doesn't even attempt an answer. It's content just going along for the ride. It's too busy having fun.\n",
      "\n",
      "Franklin relies too much on easy laugh-getters such as flatulence and crotch-kick gags, but at least he does provide his half-baked script with a fairly satisfying wrap-up.\n",
      "\n",
      "Though it's a good half hour too long, this overblown 1993 spin-off of the 60s TV show otherwise adds up to a pretty good suspense thriller.\n",
      "\n",
      "I saw this at a festival and hated it, then sat through it again a year later and decided it wasn't so bad, aside from the god-awful ending.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = make_xy(critics, vectorizer)\n",
    "\n",
    "prob = clf.predict_proba(x)[:, 0]\n",
    "predict = clf.predict(x)\n",
    "\n",
    "bad_rotten = np.argsort(prob[y == 0])[:5]\n",
    "bad_fresh = np.argsort(prob[y == 1])[-5:]\n",
    "\n",
    "print(\"Mis-predicted Rotten quotes\")\n",
    "print('---------------------------')\n",
    "for row in bad_rotten:\n",
    "    print(critics[y == 0].quote.iloc[row])\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Mis-predicted Fresh quotes\")\n",
    "print('--------------------------')\n",
    "for row in bad_fresh:\n",
    "    print(critics[y == 1].quote.iloc[row])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VII: Predicting the Freshness for a New Review</h3>\n",
    "<br/>\n",
    "<div>\n",
    "<b>Exercise:</b>\n",
    "<ul>\n",
    "<li> Using your best trained classifier, predict the freshness of the following sentence: *'This movie is not remarkable, touching, or superb in any way'*\n",
    "<li> Is the result what you'd expect? Why (not)?\n",
    "</ul>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "#your turn\n",
    "sentence = [\"This movie is not remarkable, touching, or superb in any way\"]\n",
    "vect_sentence = vectorizer.transform(sentence)\n",
    "vect_sentence = vect_sentence.tocsc()\n",
    "predict = clf.predict(vect_sentence)\n",
    "if predict[0] == 0:\n",
    "    print(\"Rotten.\")\n",
    "else:\n",
    "    print(\"Fresh.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the the tested sentence corresponds to a 'rotten' sentence, however the classifier predicts it as a 'rresh' one. So we get the opposite of what is expected.\n",
    "\n",
    "The classfier looks to the probability of each word of conveying a positive or negative feeling. For the sake of simplicity let's say that the only words transmitting a feeling are 'not',  'remarkable', touching' and 'superb', the rest of the words being neutral. The word 'not' can be associated somehow with a 'rotten' feeling and the words 'remarkable', touching' and 'superb' absolutely convey 'freshness'. As a consequence, we have three strong positive words weighted again one weak negative word, so it is not surprising that the setence was classified as 'fresh' instead of 'rotten'.\n",
    "\n",
    "The classifier considers the individual meaning of each word but not the full context of the sentence or the association between words. A strong classifier would be one capable of recognicing the association of different words. In the example above a strong classifier will recognize that 'not' is multiplying the positive words so it would be able to predict that the setence is a 'rotten' one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: TF-IDF Weighting for Term Importance\n",
    "\n",
    "TF-IDF stands for \n",
    "\n",
    "`Term-Frequency X Inverse Document Frequency`.\n",
    "\n",
    "In the standard `CountVectorizer` model above, we used just the term frequency in a document of words in our vocabulary. In TF-IDF, we weight this term frequency by the inverse of its popularity in all documents. For example, if the word \"movie\" showed up in all the documents, it would not have much predictive value. It could actually be considered a stopword. By weighing its counts by 1 divided by its overall frequency, we downweight it. We can then use this TF-IDF weighted features as inputs to any classifier. **TF-IDF is essentially a measure of term importance, and of how discriminative a word is in a corpus.** There are a variety of nuances involved in computing TF-IDF, mainly involving where to add the smoothing term to avoid division by 0, or log of 0 errors. The formula for TF-IDF in `scikit-learn` differs from that of most textbooks: \n",
    "\n",
    "$$\\mbox{TF-IDF}(t, d) = \\mbox{TF}(t, d)\\times \\mbox{IDF}(t) = n_{td} \\log{\\left( \\frac{\\vert D \\vert}{\\vert d : t \\in d \\vert} + 1 \\right)}$$\n",
    "\n",
    "where $n_{td}$ is the number of times term $t$ occurs in document $d$, $\\vert D \\vert$ is the number of documents, and $\\vert d : t \\in d \\vert$ is the number of documents that contain $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/dev/modules/feature_extraction.html#text-feature-extraction\n",
    "# http://scikit-learn.org/dev/modules/classes.html#text-feature-extraction-ref\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "Xtfidf=tfidfvectorizer.fit_transform(critics.quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VIII: Enrichment</h3>\n",
    "\n",
    "<p>\n",
    "There are several additional things we could try. Try some of these as exercises:\n",
    "<ol>\n",
    "<li> Build a Naive Bayes model where the features are n-grams instead of words. N-grams are phrases containing n words next to each other: a bigram contains 2 words, a trigram contains 3 words, and 6-gram contains 6 words. This is useful because \"not good\" and \"so good\" mean very different things. On the other hand, as n increases, the model does not scale well since the feature set becomes more sparse.\n",
    "<li> Try a model besides Naive Bayes, one that would allow for interactions between words -- for example, a Random Forest classifier.\n",
    "<li> Try adding supplemental features -- information about genre, director, cast, etc.\n",
    "<li> Use word2vec or [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to group words into topics and use those topics for prediction.\n",
    "<li> Use TF-IDF weighting instead of word counts.\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<b>Exercise:</b> Try a few of these ideas to improve the model (or any other ideas of your own). Implement here and report on the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]\n",
    "best_min_df = min_df # YOUR TURN: put your value of min_df here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_multinomial_classifier(ngram_range=(1, 1), alphas=alphas, min_df=best_min_df):\n",
    "    #Find the best value for alpha and min_df, and the best classifier\n",
    "    vectorizer = CountVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    best_alpha = None\n",
    "    maxscore=-np.inf\n",
    "    for alpha in alphas:              \n",
    "        Xthis, ythis = make_xy(critics, vectorizer)\n",
    "        Xtrainthis = Xthis[mask]\n",
    "        ytrainthis = ythis[mask]\n",
    "        # your turn\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        score = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "        if score > maxscore:\n",
    "            best_alpha = alpha\n",
    "            maxscore = score\n",
    "    print(\"alpha: {}\".format(best_alpha))\n",
    "        \n",
    "    # train_multinomial_classifier\n",
    "    X, y = make_xy(critics, vectorizer)\n",
    "    xtrain=X[mask]\n",
    "    ytrain=y[mask]\n",
    "    xtest=X[~mask]\n",
    "    ytest=y[~mask]\n",
    "\n",
    "    clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "    training_accuracy = clf.score(xtrain, ytrain)\n",
    "    test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "    print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "    print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n",
    "    \n",
    "    conf_mat = confusion_matrix(ytest, clf.predict(xtest))\n",
    "    TN = conf_mat[0, 0] #True Negatives\n",
    "    FP = conf_mat[0, 1] #False Positives\n",
    "    FN = conf_mat[1, 0] #False Negatives\n",
    "    TP = conf_mat[1, 1] #True Positives\n",
    "    precision = TP / (FP + TP)\n",
    "    recall = TP / (FN + TP)\n",
    "    print(\"Confusion matrix:\\n {}\".format(conf_mat))\n",
    "    print(\"Precision: %.2f %%\" %(precision*100))\n",
    "    print(\"Recall: %.2f %%\" %(recall*100))\n",
    "    \n",
    "    return best_alpha, vectorizer, clf\n",
    "\n",
    "def test_rotten_sentence(clf=None, sentence = [\"This movie is not remarkable, touching, or superb in any way\"], vectorizer=None):\n",
    "    vect_sentence = vectorizer.transform(sentence)\n",
    "    vect_sentence = vect_sentence.tocsc()\n",
    "    predict = clf.predict(vect_sentence)\n",
    "    if predict[0] == 0:\n",
    "        print(\"Rotten.\")\n",
    "    else:\n",
    "        print(\"Fresh.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n",
      "Accuracy on training data: 0.973656\n",
      "Accuracy on test data:     0.742288\n",
      "Confusion matrix:\n",
      " [[2274 1998]\n",
      " [ 809 5811]]\n",
      "Precision: 74.41 %\n",
      "Recall: 87.78 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,2)\n",
    "best_alpha, bigram_vectorizer, bigram_clf = ngram_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                         min_df=best_min_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=bigram_clf, vectorizer=bigram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n",
      "Accuracy on training data: 0.979010\n",
      "Accuracy on test data:     0.738799\n",
      "Confusion matrix:\n",
      " [[2255 2017]\n",
      " [ 828 5792]]\n",
      "Precision: 74.17 %\n",
      "Recall: 87.49 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,3)\n",
    "best_alpha, trigram_vectorizer, trigram_clf = ngram_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                           min_df=best_min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=trigram_clf, vectorizer=trigram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n",
      "Accuracy on training data: 0.979653\n",
      "Accuracy on test data:     0.737697\n",
      "Confusion matrix:\n",
      " [[2255 2017]\n",
      " [ 840 5780]]\n",
      "Precision: 74.13 %\n",
      "Recall: 87.31 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,6)\n",
    "best_alpha, sixgram_vectorizer, sixgram_clf = ngram_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                           min_df=best_min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=sixgram_clf, vectorizer=sixgram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n",
      "Accuracy on training data: 0.979653\n",
      "Accuracy on test data:     0.737973\n",
      "Confusion matrix:\n",
      " [[2255 2017]\n",
      " [ 837 5783]]\n",
      "Precision: 74.14 %\n",
      "Recall: 87.36 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,7)\n",
    "best_alpha, ngram_vectorizer, ngram_clf = ngram_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                       min_df=best_min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=ngram_clf, vectorizer=ngram_vectorizer) # sentence=['good is not'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_multinomial_classifier(ngram_range=(1, 1), alphas=alphas, min_df=best_min_df):\n",
    "    #Find the best value for alpha and min_df, and the best classifier\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, stop_words='english', ngram_range=ngram_range)\n",
    "    best_alpha = None\n",
    "    maxscore=-np.inf\n",
    "    for alpha in alphas:              \n",
    "        Xthis, ythis = make_xy(critics, vectorizer)\n",
    "        Xtrainthis = Xthis[mask]\n",
    "        ytrainthis = ythis[mask]\n",
    "        # your turn\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        score = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "        if score > maxscore:\n",
    "            best_alpha = alpha\n",
    "            maxscore = score\n",
    "    print(\"alpha: {}\".format(best_alpha))\n",
    "        \n",
    "    # train_multinomial_classifier\n",
    "    X, y = make_xy(critics, vectorizer)\n",
    "    xtrain=X[mask]\n",
    "    ytrain=y[mask]\n",
    "    xtest=X[~mask]\n",
    "    ytest=y[~mask]\n",
    "\n",
    "    clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "    training_accuracy = clf.score(xtrain, ytrain)\n",
    "    test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "    print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "    print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n",
    "    \n",
    "    conf_mat = confusion_matrix(ytest, clf.predict(xtest))\n",
    "    TN = conf_mat[0, 0] #True Negatives\n",
    "    FP = conf_mat[0, 1] #False Positives\n",
    "    FN = conf_mat[1, 0] #False Negatives\n",
    "    TP = conf_mat[1, 1] #True Positives\n",
    "    precision = TP / (FP + TP)\n",
    "    recall = TP / (FN + TP)\n",
    "    print(\"Confusion matrix:\\n {}\".format(conf_mat))\n",
    "    print(\"Precision: %.2f %%\" %(precision*100))\n",
    "    print(\"Recall: %.2f %%\" %(recall*100))    \n",
    "    \n",
    "    return best_alpha, vectorizer, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n",
      "Accuracy on training data: 0.975369\n",
      "Accuracy on test data:     0.722640\n",
      "Confusion matrix:\n",
      " [[2192 2080]\n",
      " [ 941 5679]]\n",
      "Precision: 73.19 %\n",
      "Recall: 85.79 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,1)\n",
    "best_alpha, tfid_vectorizer, tfid_clf = tfid_multinomial_classifier(ngram_range=ngram_range, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=tfid_clf, vectorizer=tfid_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n",
      "Accuracy on training data: 0.998715\n",
      "Accuracy on test data:     0.718142\n",
      "Confusion matrix:\n",
      " [[2062 2210]\n",
      " [ 860 5760]]\n",
      "Precision: 72.27 %\n",
      "Recall: 87.01 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,2)\n",
    "best_alpha, tfid_2gram_vectorizer, tfid_2gram_clf = tfid_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                                min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=tfid_2gram_clf, vectorizer=tfid_2gram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n",
      "Accuracy on training data: 0.999143\n",
      "Accuracy on test data:     0.716214\n",
      "Confusion matrix:\n",
      " [[2045 2227]\n",
      " [ 864 5756]]\n",
      "Precision: 72.10 %\n",
      "Recall: 86.95 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,3)\n",
    "best_alpha, tfid_3gram_vectorizer, tfid_3gram_clf = tfid_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                                min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=tfid_3gram_clf, vectorizer=tfid_3gram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 6-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n",
      "Accuracy on training data: 0.999143\n",
      "Accuracy on test data:     0.707951\n",
      "Confusion matrix:\n",
      " [[2121 2151]\n",
      " [1030 5590]]\n",
      "Precision: 72.21 %\n",
      "Recall: 84.44 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,6)\n",
    "best_alpha, tfid_6gram_vectorizer, tfid_6gram_clf = tfid_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                                min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=tfid_6gram_clf, vectorizer=tfid_6gram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n",
      "Accuracy on training data: 0.999143\n",
      "Accuracy on test data:     0.716214\n",
      "Confusion matrix:\n",
      " [[2045 2227]\n",
      " [ 864 5756]]\n",
      "Precision: 72.10 %\n",
      "Recall: 86.95 %\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,3)\n",
    "best_alpha, tfid_gram_vectorizer, tfid_gram_clf = tfid_multinomial_classifier(ngram_range=ngram_range, \\\n",
    "                                                                              min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotten.\n"
     ]
    }
   ],
   "source": [
    "test_rotten_sentence(clf=tfid_gram_clf, vectorizer=tfid_gram_vectorizer, sentence=[\"good isn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
